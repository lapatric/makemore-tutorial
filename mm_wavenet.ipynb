{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task**\n",
    "\n",
    "We want to train an MLP to learn the construction of names. To achieve this we want to maximise the likelihood of the $N$ names observed in the dataset:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\sum_{i=1}^{N} \\hat{p}(\\mathbf{x}_i) = \\max_{\\theta} \\sum_{i=1}^{N} \\prod_{j=1}^{M_i} \\hat{p}(x_j | x_{j-1}, x_{j-2}, x_{j-3}; \\theta)\n",
    "$$\n",
    "\n",
    "In particular, we choose to break down the probability of a name into the product of the conditional probabilities of each character given a three-letter context window (we use '.' as the special token to pad the start and end of a name). For example, the name \"John\" would give us the following data points:\n",
    "\n",
    "* ... $\\rightarrow$ J\n",
    "* ..J $\\rightarrow$ o\n",
    "* .Jo $\\rightarrow$ h\n",
    "* Joh $\\rightarrow$ n\n",
    "* ohn $\\rightarrow$ .\n",
    "\n",
    "We begin by constructing a training set of such data points from all the names in the dataset. We then use the negative log-likelihood of the data as the loss function to be minimized. See `mm_intro.ipynb` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps characters to integers and vice versa\n",
    "char2idx = {c: i+1 for i, c in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "char2idx['.'] = 0 # special character for marking start and end of a word\n",
    "idx2char = {i: c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625]) torch.int64 torch.int64\n",
      "torch.Size([22655, 3]) torch.Size([22655]) torch.int64 torch.int64\n",
      "torch.Size([22866, 3]) torch.Size([22866]) torch.int64 torch.int64\n",
      "... -> y\n",
      "..y -> u\n",
      ".yu -> h\n",
      "yuh -> e\n",
      "uhe -> n\n",
      "hen -> g\n",
      "eng -> .\n",
      "... -> d\n",
      "..d -> i\n",
      ".di -> o\n"
     ]
    }
   ],
   "source": [
    "# Form training pairs (context, target characters)\n",
    "block_size = 3 # context size for next character prediction\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        w2idx = [0] * block_size + [char2idx[c] for c in word] + [0]\n",
    "        for i in range(len(w2idx) - block_size):\n",
    "            X.append(w2idx[i:i+block_size])\n",
    "            Y.append(w2idx[i+block_size])\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape, X.dtype, Y.dtype)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "X_train, Y_train = build_dataset(words[:n1]) # 80% of words\n",
    "X_val, Y_val = build_dataset(words[n1:n2]) # 10% of words\n",
    "X_test, Y_test = build_dataset(words[n2:]) # 10% of words\n",
    "\n",
    "# print first 10 samples of X_train and Y_train\n",
    "for i in range(len(Y_train[:10])):\n",
    "    print(''.join([idx2char[idx.item()] for idx in X_train[i]]), '->', idx2char[Y_train[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**\n",
    "\n",
    "We begin by implementing PyTorch-like modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, emb_dim):\n",
    "        self.W = torch.randn((num_embeddings, emb_dim)) # / dim**0.5\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = self.W[x]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.W]\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.W = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.b = torch.zeros((fan_out)) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.W\n",
    "        if self.b is not None:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        if self.b is not None:\n",
    "            return [self.W, self.b]\n",
    "        else:\n",
    "            return [self.W]\n",
    "        \n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.gain = torch.ones((dim))\n",
    "        self.bias = torch.zeros((dim))\n",
    "        self.eps = eps\n",
    "        self.training = True\n",
    "        self.momentum = momentum\n",
    "        self.mean_running = torch.zeros((dim))\n",
    "        self.var_running = torch.ones((dim))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            x_mean = x.mean(dim=0, keepdim=True)\n",
    "            x_var = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            x_mean = self.mean_running\n",
    "            x_var = self.var_running\n",
    "        x_hat = (x - x_mean) / (x_var + self.eps).sqrt()\n",
    "        self.out = self.gain * x_hat + self.bias\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.mean_running = (1 - self.momentum) * self.mean_running + self.momentum * x_mean\n",
    "                self.var_running = (1 - self.momentum) * self.var_running + self.momentum * x_var\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gain, self.bias]\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42); # seed RNG for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12097\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 10 # embedding dimension\n",
    "h_dim = 200 # hidden layer dimension\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(27, emb_dim), Flatten(),\n",
    "    Linear(block_size * emb_dim, h_dim, bias=False), BatchNorm1d(h_dim), Tanh(), \n",
    "    Linear(h_dim, 27)\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].W *= 0.1 # make last linear layer less confident (more uniform) predictions\n",
    "\n",
    "parameters = model.parameters()\n",
    "print('Number of parameters:', sum([p.numel() for p in parameters]))\n",
    "\n",
    "# Always perform last \n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "loss_i = []\n",
    "upd2data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3121\n",
      "   1000/ 200000: 2.6946\n",
      "   2000/ 200000: 2.4040\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for i in range(max_steps):\n",
    "    # mini-batch sampling\n",
    "    batch_idxs = torch.randperm(len(Y_train))[:batch_size]\n",
    "    X_batch, Y_batch = X_train[batch_idxs], Y_train[batch_idxs]\n",
    "  \n",
    "    # forward pass\n",
    "    logits = model(X_batch)\n",
    "    loss = F.cross_entropy(logits, Y_batch)\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    lr = 0.1 if i < 150000 else 0.01 # decrease learning rate after 150K steps\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    loss_i.append(loss.log10().item())\n",
    "    # with torch.no_grad():\n",
    "    #     upd2data.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1000]' is invalid for input of size 2001",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We smoothen out the loss curve by averaging over 1000 steps\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Due to the small batch size of 32, it would be too noisy otherwise\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(torch\u001b[39m.\u001b[39;49mtensor(loss_i)\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1000\u001b[39;49m)\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1000]' is invalid for input of size 2001"
     ]
    }
   ],
   "source": [
    "# We smoothen out the loss curve by averaging over 1000 steps\n",
    "# Due to the small batch size of 32, it would be too noisy otherwise\n",
    "plt.plot(torch.tensor(loss_i).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for batch norm especially)\n",
    "for l in model.layers:\n",
    "    l.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.372546434402466\n",
      "Loss: 2.3663876056671143\n"
     ]
    }
   ],
   "source": [
    "# forward pass for loss validation\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_losss(split):\n",
    "    x, y = {\n",
    "        'train': (X_train, Y_train),\n",
    "        'val': (X_val, Y_val),\n",
    "        'test': (X_test, Y_test)\n",
    "    }[split] \n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y) \n",
    "    print('Loss:', loss.item())\n",
    "\n",
    "split_losss('train')\n",
    "split_losss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# sample next character\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m i \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probs, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m context \u001b[39m=\u001b[39m context[\u001b[39m1\u001b[39m:] \u001b[39m+\u001b[39m [i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Documents/AI/makemore/mm_wavenet.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m name \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m idx2char[i]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    name = ''\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample next character\n",
    "        i = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [i]\n",
    "        name += idx2char[i]\n",
    "        if i == 0: \n",
    "            break\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Activations\n",
    "\n",
    "Now, we visualize both the activations and their gradients of the hidden layers after one iteration to see if there are any issues. The main thing to look out for is that our activations are not overly saturated or too inactive. Furthermore, if the gradients through the activation nodes are too small or too big, then the model will not be able to learn effectively either. Overall, it's important to have a similar distribution of activations and gradients across all layers of the model to allow for optimal gradient flow and learning. If the distribution of activations and gradients changes significantly across the layers, i.e. there is poor congruence across their distributions, then we must investigate whether our initializations are appropriate or whether we need to introduce some form of normalization such as BatchNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activations \n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, l in enumerate(layers):\n",
    "    if isinstance(l, Tanh):\n",
    "        t = l.out\n",
    "        print('layer %d (%10s): mean %.2f, std %.2f, saturated: %.2f%%' % (i, l.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean() * 100))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({l.__class__.__name__})')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation gradients\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, l in enumerate(layers):\n",
    "    if isinstance(l, Tanh):\n",
    "        t = l.out.grad\n",
    "        print('layer %d (%10s): mean %+f, std %e' % (i, l.__class__.__name__, t.mean(), t.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({l.__class__.__name__})')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Weights \n",
    "\n",
    "In addition to analysing the activations and their gradients, we also want to analyse the weights and visualize their distribution. Similar to the activations, we want to see a similar distribution of weights across all layers of the model. If certain weights are too large relative to others, then the gradients flowing through these weights will also be large and cause these areas to learn faster than others (assuming a simple optimzation approach such as SGD). This may lead to the model getting stuck in a local minimum. \n",
    "\n",
    "Finally, we also study the gradient-to-weight ratio (\"grad:data ratio\") which is a measure of how much the weights change in response to a change in the loss. If the ratio is too large, then the weights will change too much and the model will not be able to learn effectively. If the ratio is too small, then the weights will not change enough and the model will also not be able to learn effectively. Naturally, this also depends on the learning rate so we must put this into context, which is why we create the second plot below showing the actual _update to weight ratio_ which takes the learning rate into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight gradient distribution\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    t = p.grad\n",
    "    if p.ndim == 2:\n",
    "        # Note that we compute the grad:data ration using the std, not the mean as the mean may be close to zero \n",
    "        # wheras the std gives us a good idea of the scale of the gradients and the data\n",
    "        print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends);\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update to Weight Ratio\n",
    "\n",
    "In general, our _update to weight ratios_ for each of the layers look good and lie around the \"optimal\" value of $1\\mathrm{e}{-3}$. However, we do observe a slight outlier in the the output layer which has a much larger ratio. This is because we scaled the weights of the output layer by a factor of $0.1$ in order to bring the initialisation of the weights closer to zero and hence reduce the random overconfidence of the model for one particular character (essentially bringing it closer to uniform distribution). We observe that this outlier stabilizes after a few iterations and the ratio is brought down to a more reasonable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot update to weight (data) ratio\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([upd2data[j][i] for j in range(len(upd2data))])\n",
    "        legends.append('param %d' % i)\n",
    "plt.plot([0, len(upd2data)], [-3, -3], 'k') # guideline, the ratios should be ~1e-3\n",
    "plt.legend(legends);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
