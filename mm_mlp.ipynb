{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task**\n",
    "\n",
    "We want to train an MLP to learn the construction of names. To achieve this we want to maximise the likelihood of the $N$ names observed in the dataset:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\sum_{i=1}^{N} \\hat{p}(\\mathbf{x}_i) = \\max_{\\theta} \\sum_{i=1}^{N} \\prod_{j=1}^{M_i} \\hat{p}(x_j | x_{j-1}, x_{j-2}, x_{j-3}; \\theta)\n",
    "$$\n",
    "\n",
    "In particular, we choose to break down the probability of a name into the product of the conditional probabilities of each character given a three-letter context window (we use '.' as the special token to pad the start and end of a name). For example, the name \"John\" would give us the following data points:\n",
    "\n",
    "* ... $\\rightarrow$ J\n",
    "* ..J $\\rightarrow$ o\n",
    "* .Jo $\\rightarrow$ h\n",
    "* Joh $\\rightarrow$ n\n",
    "* ohn $\\rightarrow$ .\n",
    "\n",
    "We begin by constructing a training set of such data points from all the names in the dataset. We then use the negative log-likelihood of the data as the loss function to be minimized. See `mm_intro.ipynb` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps characters to integers and vice versa\n",
    "char2idx = {c: i+1 for i, c in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "char2idx['.'] = 0 # special character for marking start and end of a word\n",
    "idx2char = {i: c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([32, 3]) torch.int64 \n",
      "Y: torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Form training pairs of context and target characters\n",
    "block_size = 3 # context size for next character prediction\n",
    "X, Y = [], []\n",
    "\n",
    "for word in words[:5]:\n",
    "    w2idx = [0] * block_size + [char2idx[c] for c in word] + [0]\n",
    "    for i in range(len(w2idx) - block_size):\n",
    "        X.append(w2idx[i:i+block_size])\n",
    "        Y.append(w2idx[i+block_size])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print('X:', X.shape, X.dtype, '\\nY:', Y.shape, Y.dtype)\n",
    "# print first 5 samples of X and Y\n",
    "# for i in range(len(Y)):\n",
    "#     print(''.join([idx2char[idx.item()] for idx in X[i]]), '->', idx2char[Y[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
