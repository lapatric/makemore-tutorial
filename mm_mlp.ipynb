{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task**\n",
    "\n",
    "We want to train an MLP to learn the construction of names. To achieve this we want to maximise the likelihood of the $N$ names observed in the dataset:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\sum_{i=1}^{N} \\hat{p}(\\mathbf{x}_i) = \\max_{\\theta} \\sum_{i=1}^{N} \\prod_{j=1}^{M_i} \\hat{p}(x_j | x_{j-1}, x_{j-2}, x_{j-3}; \\theta)\n",
    "$$\n",
    "\n",
    "In particular, we choose to break down the probability of a name into the product of the conditional probabilities of each character given a three-letter context window (we use '.' as the special token to pad the start and end of a name). For example, the name \"John\" would give us the following data points:\n",
    "\n",
    "* ... $\\rightarrow$ J\n",
    "* ..J $\\rightarrow$ o\n",
    "* .Jo $\\rightarrow$ h\n",
    "* Joh $\\rightarrow$ n\n",
    "* ohn $\\rightarrow$ .\n",
    "\n",
    "We begin by constructing a training set of such data points from all the names in the dataset. We then use the negative log-likelihood of the data as the loss function to be minimized. See `mm_intro.ipynb` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps characters to integers and vice versa\n",
    "char2idx = {c: i+1 for i, c in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "char2idx['.'] = 0 # special character for marking start and end of a word\n",
    "idx2char = {i: c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 3]) torch.Size([182580]) torch.int64 torch.int64\n",
      "torch.Size([22767, 3]) torch.Size([22767]) torch.int64 torch.int64\n",
      "torch.Size([22799, 3]) torch.Size([22799]) torch.int64 torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Form training pairs (context, target characters)\n",
    "block_size = 3 # context size for next character prediction\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        w2idx = [0] * block_size + [char2idx[c] for c in word] + [0]\n",
    "        for i in range(len(w2idx) - block_size):\n",
    "            X.append(w2idx[i:i+block_size])\n",
    "            Y.append(w2idx[i+block_size])\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape, X.dtype, Y.dtype)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "X_train, Y_train = build_dataset(words[:n1]) # 80% of words\n",
    "X_val, Y_val = build_dataset(words[n1:n2]) # 10% of words\n",
    "X_test, Y_test = build_dataset(words[n2:]) # 10% of words\n",
    "\n",
    "# print first 10 samples of X_train and Y_train\n",
    "# for i in range(len(Y_train[:10])):\n",
    "#     print(''.join([idx2char[idx.item()] for idx in X_train[i]]), '->', idx2char[Y_train[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLP**\n",
    "\n",
    "We will use a simple MLP with an embedding layer for the input (shared across all characters), one hidden layer, and a softmax output layer. The approach is similar to the one used in the paper [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).\n",
    "\n",
    "1. **Embedding Layer**: As done in `mm_intro.ipynb`, characters are represented as one-hot vectors and passed through an embedding layer to obtain a dense representation. Regarding the implementation, we observe that a matrix multiplication of one-hot vectors with an embedding matrix is equivalent to a lookup in the embedding matrix. This can be implemented using simple indexing operations in PyTorch which is a lot more efficient than matrix multiplication: `emb = C[X]`.\n",
    "2. **Hidden Layer**: The hidden layer is a fully connected layer with weights and biases. The input to the hidden layer is the concatenation of the embeddings of the three characters in the context window. To perform this concatenation, we use the `view` function in PyTorch which is extremely efficient as it doesn't directly manipulate the data in memory but simply changes its interpretation. \n",
    "3. **Output Layer**: The output layer is a fully connected layer with weights and biases. The output of the hidden layer is passed through this layer to obtain the logits for each character in the vocabulary. \n",
    "4. **Loss**: Instead of manually implementing the softmax and cross-entropy loss to compute the negative log-likelihood from the logits as we did in `mm_intro.ipynb`, we use PyTorch's `torch.nn.functional.cross_entropy` function which combines the two operations into one and is numerically not only more stable but also more efficient in the forward and backward passes. This is because the softmax and cross-entropy operations are fused into a single kernel and computed together. For more info, refer to the [YoutTube video](https://youtu.be/TCH_1BHY58I?t=1968) associated with this tutorial.\n",
    "\n",
    "Note that when implementing models in PyTorch, it's always important to double-check the shapes of the tensors at each step using a small batch of data to make sure that the operations are being performed as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 11897\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 10 # embedding dimension\n",
    "h1_dim = 200 # hidden layer dimension\n",
    "\n",
    "# For reproducibility\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Embedding layer\n",
    "C = torch.randn((27, emb_dim), generator=g) # shape: (vocab_size, emb_dim)\n",
    "# Hidden layer\n",
    "W1 = torch.randn((block_size * emb_dim, h1_dim), generator=g) # shape: (block_size * emb_dim, h1_dim)\n",
    "b1 = torch.randn((h1_dim), generator=g) # shape: (h1_dim)\n",
    "# Output layer\n",
    "W2 = torch.randn((h1_dim, 27), generator=g) * 0.01 # shape: (h1_dim, vocab_size)\n",
    "b2 = torch.zeros((27)) # shape: (vocab_size)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print('Number of parameters:', sum([p.numel() for p in parameters]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**\n",
    "\n",
    "Once you have built your model it's good practice to check that it's working as expected by overfitting it on a small batch of data. This means that the model should be able to memorize the data and achieve a loss of 0. \n",
    "\n",
    "In our case, we overfit the model to the first five entries of our `words` dataset by rerunning the training set construction code above with `words[:5]`. We then train the model for 1000 epochs and observe that the loss does indeed decrease significantly (to $0.2535$). The reason, however, why we are not able to achieve a loss of $0$, despite having a relatively large model of $3481$ parameters for only five words (32 samples), is because in some cases there is more than one possible next character given the context window. For example, in the 32 samples, the context window \"...\" appears multiple times and is followed by different characters. Hence, there is some inherent ambiguity in the data which prevents the model from achieving a loss of $0$.\n",
    "\n",
    "To compare the model's predictions with our labels `Y`, we can use the `torch.argmax` function along the first dimension of the `logits` to obtain the index of the character with the highest probability and compare the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "When training a model, it's important to pick the right learning rate. If the learning rate is too small, the model will take a long time to converge. If the learning rate is too large, the model will diverge and the loss will increase. One simple approach to find the optimal learning rate is to test a range of learning rates and pick the one that achieves the lowest loss. In particular, we may choose to test $1000$ learning rates logarithmically over the range $[1\\mathrm{e}{-3}, 1]$. For each learning rate we run a single batch of training and record the loss. We then plot the loss against the learning rate and pick the learning rate that achieves the lowest loss, `plt.plot(lr_i, loss_i)`. After training our model for a sufficient number of epochs at this optimal learning rate, we can additionally perform a _learning rate decay_ (e.g. $\\times 0.1$) to potentially achieve an even lower loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat tracking\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10 ** lre # log scale LRs over range [1e-3, 1]\n",
    "lr_i = []\n",
    "loss_i = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_steps = 50000\n",
    "\n",
    "# Training loop\n",
    "for i in range(max_steps):\n",
    "    # mini-batch sampling\n",
    "    batch_idxs = torch.randperm(len(Y_train))[:batch_size]\n",
    "  \n",
    "    # forward pass\n",
    "    emb = C[X_train[batch_idxs]] # shape: (#samples, block_size, emb_dim) \n",
    "    h = torch.tanh(emb.view(-1, block_size * emb_dim) @ W1 + b1) # shape: (#samples, hidden_dim) \n",
    "    logits = h @ W2 + b2 # shape: (#samples, vocab_size)\n",
    "    loss = F.cross_entropy(logits, Y_train[batch_idxs]) # shape: (1)\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    lr = 0.01 # during LR tuning, use lrs[i] instead\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    # lr_i.append(lre[i])\n",
    "    loss_i.append(loss.log10().item())\n",
    "\n",
    "# Plot loss vs. learning rate\n",
    "# plt.plot(lr_i, loss_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ea5dcf190>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A thick band of the loss curve indicates that our batch size may be too small, i.e. the gradients are too noisy.\n",
    "plt.plot([10 ** v for v in loss_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "\n",
    "Another important aspect of training a model is to initialize the weights of the model properly. This can have a significant impact on the convergence of the model. It is often the case that for any given task we're solving, we know approximately what the scale of the loss should be to begin with. For example, in our case, we know that the loss should be around $-\\log(1/27) \\approx 3.2958$ (assuming a uniform distribution over the vocabulary to start). Hence, we can initialize the weights of the model such that the loss is around this value. In particular, let's try to initialize the weights of the output layer using a normal distribution with mean $0$ and standard deviation $\\sigma = 0.01$ and set the biases to $0$. This is because for the outputs (logits), we don't want to be adding any unnecessary bias which would shift the distribution away from the uniform distribution. \n",
    "\n",
    "```\n",
    "W2 = torch.randn((h1_dim, 27), generator=g) * 0.01 \n",
    "b2 = torch.zeros((27))\n",
    "```\n",
    "\n",
    "When we improve the initialization of these weights, we observe that the initial loss in the first training iteration is indeed a lot lower than before and around the expected value of $3.2958$. As a consequence of this improved weight inititialization, we observe that the loss curve doesn't resemble a hockey stick anymore and the model is able to spend more time in the region of low loss, improving the convergence of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Once we have trained our model, we want to evaluate its performance on a validation set. In particular, we want to check that the model is not overfitting to the training set and that it is able to generalize to unseen data. To do this, we compute the loss on the validation set and compare it to the loss on the training set. If the loss on the validation set is significantly higher than the loss on the training set, then the model is overfitting. In this case, we may want to consider using a smaller model or adding regularization techniques such as dropout. However, if the loss on the validation set is similar to the loss on the training set, then the model is underfitting and can expect to achieve better performance by using a larger model.\n",
    "\n",
    "The trick when adapting the model to these findings is to do it systematically. We may begin increasing the size of one layer and be aware of bottlenecks that may arise due to the size of the other layers not being large enough. We may also want to consider adding more layers to the model. In general, it's good practice to start with a small model and gradually increase its size until we start to see diminishing returns. In our case we started with an embedding size of $2$ per character and a hidden layer of size $100$. We then increased the embedding size to $10$ and the hidden layer size to $200$ to achieve a validation loss of $2.1983$ and a training loss of $2.1703$, when training for $50\\text{k}$ steps with $\\text{LR}{=}0.1$ and another $50\\text{k}$ steps with $\\text{LR}{=}0.01$. Batch size was $32$. At this point our model may be big enough where it's starting to overfit to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0995891094207764\n",
      "Loss: 2.1471335887908936\n"
     ]
    }
   ],
   "source": [
    "# forward pass for validation / testing\n",
    "def split_losss(split):\n",
    "    X, Y = {\n",
    "        'train': (X_train, Y_train),\n",
    "        'val': (X_val, Y_val),\n",
    "        'test': (X_test, Y_test)\n",
    "    }[split] \n",
    "    with torch.no_grad():\n",
    "        emb = C[X] # shape: (#samples, block_size, emb_dim) \n",
    "        h = torch.tanh(emb.view(-1, block_size * emb_dim) @ W1 + b1) # shape: (#samples, hidden_dim) \n",
    "        logits = h @ W2 + b2 # shape: (#samples, vocab_size)\n",
    "        loss = F.cross_entropy(logits, Y) # shape: (1)\n",
    "        print('Loss:', loss.item())\n",
    "\n",
    "split_losss('train')\n",
    "split_losss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We can plot the embeddings of the characters in our vocabulary to see if there are any interesting patterns. For example, we may observe that vowels are clustered together. Furthermore, some of the more rare letters such as 'q' form outliers. As characters that are clustered together have similar embeddings, they may be interchangeable in some contexts. For example, the names \"John\" and \"Jahn\" are both valid names and the model should be able to predict the next character correctly in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently only works for 2D embeddings\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), idx2char[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Once we have trained our model, we can use it to generate new names. To do this, we first feed the model a context window of three characters and sample the next character from the distribution over the vocabulary. We then shift the context window by one character and repeat the process. We continue this process until we sample the special token '.' which indicates the end of a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ziti.\n",
      "joretty.\n",
      "con.\n",
      "reon.\n",
      "isa.\n",
      "iri.\n",
      "evondralla.\n",
      "ortayshith.\n",
      "desiah.\n",
      "alings.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    name = ''\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[context] \n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1) \n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample next character\n",
    "        i = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [i]\n",
    "        name += idx2char[i]\n",
    "        if i == 0: \n",
    "            break\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
