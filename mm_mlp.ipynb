{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset**\n",
    "\n",
    "We want to train an MLP to learn the conditional distribution of characters in names w.r.t. a 3-letter context window, i.e. $P(x_i|x_{i-1}, x_{i-2}, x_{i-3})$. To achieve this we take each name in the dataset and extract from it all possible three-letter sequences and the corresponding target letter.\n",
    "We use '.' as the special token to denote the start and end of a name. For example, the name \"John\" would give us the following data points:\n",
    "\n",
    "* ... $\\rightarrow$ J\n",
    "* ..J $\\rightarrow$ o\n",
    "* .Jo $\\rightarrow$ h\n",
    "* Joh $\\rightarrow$ n\n",
    "* ohn $\\rightarrow$ .\n",
    "\n",
    "The objective of the network then is to maximize the log-likelihood of the data, i.e. to maximize the probability of the target letter given the context window. We use the negative log-likelihood as the loss function to be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps characters to integers and vice versa\n",
    "char2idx = {c: i+1 for i, c in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "char2idx['.'] = 0 # special character for marking start and end of a word\n",
    "idx2char = {i: c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([32, 3]) torch.int64 \n",
      "Y: torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Form training pairs of context and target characters\n",
    "block_size = 3 # context size for next character prediction\n",
    "X, Y = [], []\n",
    "\n",
    "for word in words[:5]:\n",
    "    w2idx = [0] * block_size + [char2idx[c] for c in word] + [0]\n",
    "    for i in range(len(w2idx) - block_size):\n",
    "        X.append(w2idx[i:i+block_size])\n",
    "        Y.append(w2idx[i+block_size])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print('X:', X.shape, X.dtype, '\\nY:', Y.shape, Y.dtype)\n",
    "# print first 5 samples of X and Y\n",
    "# for i in range(len(Y)):\n",
    "#     print(''.join([idx2char[idx.item()] for idx in X[i]]), '->', idx2char[Y[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
